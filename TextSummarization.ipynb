{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled24.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMWl6uWV4G1Gsj6DGwP58GT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hirva-Bhagat/NLP_Practical_Project/blob/main/TextSummarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Qr-KkZg5Un4",
        "outputId": "19f30c28-29e8-4e6c-9238-810f53cbd41f"
      },
      "source": [
        "from nltk.corpus import stopwords\r\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\r\n",
        "from string import punctuation\r\n",
        "from nltk.stem.snowball import SnowballStemmer\r\n",
        "import nltk\r\n",
        "nltk.download('stopwords')\r\n",
        "import urllib.request\r\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGOgjlPY9Max"
      },
      "source": [
        "class FrequencySummarizer:\r\n",
        "     def __init__(self, min_cut=0.1, max_cut=0.9):\r\n",
        "\r\n",
        "          self.min_cut = min_cut\r\n",
        "          self.max_cut = max_cut\r\n",
        "          self.stopwords = set(stopwords.words(\"english\")+ list(punctuation))\r\n",
        "          \r\n",
        "     def _ComputeFrequency(self, word_sent):\r\n",
        "          freq = defaultdict(int)\r\n",
        "          for s in word_sent:\r\n",
        "               for word in s:\r\n",
        "                    if word not in self.stopwords:\r\n",
        "                         freq[word] +=1\r\n",
        "\r\n",
        "          # frequencies normalization and filtering\r\n",
        "          m = float(max(freq.values()))\r\n",
        "          for w in freq.keys():\r\n",
        "               freq[w] = freq[w]/m\r\n",
        "               if freq[w] >= self.max_cut or freq[w] <= self.min_cut:\r\n",
        "                    del freq[w]\r\n",
        "               return freq\r\n",
        "\r\n",
        "     def Summarize(self, text, n):\r\n",
        "\r\n",
        "          sents = sent_tokenize(text)\r\n",
        "          assert n <= len(sents)\r\n",
        "          word_sent = [word_tokenize(s.lower()) for s in sents]\r\n",
        "          self._freq = self._ComputeFrequency(word_sent)\r\n",
        "          ranking = defaultdict(int)\r\n",
        "          for i,sent in enumerate(word_sent):\r\n",
        "               for w in sent:\r\n",
        "                    if w in self._freq:\r\n",
        "                         ranking[i] += self._freq[w]\r\n",
        "\r\n",
        "          sents_idx = self._Rank(ranking,n)\r\n",
        "          return [sents[j] for j in sents_idx]\r\n",
        "\r\n",
        "     def _Rank(self, ranking, n):\r\n",
        "              return nlargest(n, ranking, key=ranking.get)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nbraj5sw5aw4"
      },
      "source": [
        "class TextSummarizer:\r\n",
        "     stemmer = SnowballStemmer(\"english\")\r\n",
        "     stopWords = set(stopwords.words(\"english\")+ list(punctuation))\r\n",
        "     text = \"\"\r\n",
        "     sentences = \"\"\r\n",
        "     def TokenizeSentance(self):\r\n",
        "          words = word_tokenize(self.text)\r\n",
        "          print(words)\r\n",
        "          return words;\r\n",
        "\r\n",
        "     def input_text(self):\r\n",
        "          \r\n",
        "          while True:\r\n",
        "               self.text = input(\"Enter the text to summarize:\\n\")\r\n",
        "               if(len(self.text)>10):\r\n",
        "                    break;\r\n",
        "               else:\r\n",
        "                    print(\"Need text of Length greater than 10\")\r\n",
        "\r\n",
        "     \r\n",
        "     def FrequencyCalculate(self,words):\r\n",
        "\r\n",
        "          freqTable = dict()\r\n",
        "          for word in words:\r\n",
        "               word = word.lower()\r\n",
        "               if word in self.stopWords:\r\n",
        "                    continue\r\n",
        "               \r\n",
        "               if word in freqTable:\r\n",
        "                    freqTable[word] += 1\r\n",
        "               else:\r\n",
        "                    freqTable[word] = 1\r\n",
        "          return freqTable;\r\n",
        "\r\n",
        "\r\n",
        "     def SentanceComputation(self,freqTable):\r\n",
        "          \r\n",
        "          self.sentences = sent_tokenize(self.text)\r\n",
        "          sentenceValue = dict()\r\n",
        "\r\n",
        "          for sentence in self.sentences:\r\n",
        "               \r\n",
        "               for index, wordValue in enumerate(freqTable, start=1):\r\n",
        "                    \r\n",
        "                    if wordValue in sentence.lower(): \r\n",
        "                         if sentence in sentenceValue:\r\n",
        "                              \r\n",
        "                              sentenceValue[sentence] += index\r\n",
        "                         else:\r\n",
        "                              sentenceValue[sentence] = index\r\n",
        "\r\n",
        "          \r\n",
        "          print(sentenceValue)\r\n",
        "          return sentenceValue;\r\n",
        "         \r\n",
        "           \r\n",
        "\r\n",
        "     def SumAverage(self,sentenceValue):\r\n",
        "          sumValues = 0\r\n",
        "          for sentence in sentenceValue:\r\n",
        "               \r\n",
        "               sumValues += sentenceValue[sentence]\r\n",
        "\r\n",
        "          average = int(sumValues / len(sentenceValue))\r\n",
        "\r\n",
        "          return average;\r\n",
        "\r\n",
        "\r\n",
        "     def PrintSummary(self,sentenceValue,average):\r\n",
        "          summary = ''\r\n",
        "          for sentence in self.sentences:\r\n",
        "               if (sentence in sentenceValue) and (sentenceValue[sentence] > (1.5 * average)):\r\n",
        "                    summary += \" \" + sentence\r\n",
        "          return summary\r\n",
        "               "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3c3I6x2b64c3",
        "outputId": "c8ff342c-0892-49c0-8c82-130b97df6fc2"
      },
      "source": [
        "nltk.download('punkt')\r\n",
        "ts = TextSummarizer()\r\n",
        "ts.input_text()\r\n",
        "words = ts.TokenizeSentance()\r\n",
        "freqTable = ts.FrequencyCalculate(words)\r\n",
        "sentenceValue = ts.SentanceComputation(freqTable)\r\n",
        "avg = ts.SumAverage(sentenceValue)\r\n",
        "summary = ts.PrintSummary(sentenceValue,avg)\r\n",
        "print(\"\\n\\nSummary of Text:\\n\\n\")\r\n",
        "print(summary)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Enter the text to summarize:\n",
            "The importance of data to the current world can never be undermined. As per IDC, the total amount of data that is expected to sprout by 2025 is 180 zettabytes. There is no need to emphasize on the importance of text summarization in the age of big data. The algorithms or techniques that shorten the longer text and delivers that accurate but brief information are much needed. No need to say that, Text summarization will reduce the reading time, will be helpful in research and will help in finding more information in less time.\n",
            "['The', 'importance', 'of', 'data', 'to', 'the', 'current', 'world', 'can', 'never', 'be', 'undermined', '.', 'As', 'per', 'IDC', ',', 'the', 'total', 'amount', 'of', 'data', 'that', 'is', 'expected', 'to', 'sprout', 'by', '2025', 'is', '180', 'zettabytes', '.', 'There', 'is', 'no', 'need', 'to', 'emphasize', 'on', 'the', 'importance', 'of', 'text', 'summarization', 'in', 'the', 'age', 'of', 'big', 'data', '.', 'The', 'algorithms', 'or', 'techniques', 'that', 'shorten', 'the', 'longer', 'text', 'and', 'delivers', 'that', 'accurate', 'but', 'brief', 'information', 'are', 'much', 'needed', '.', 'No', 'need', 'to', 'say', 'that', ',', 'Text', 'summarization', 'will', 'reduce', 'the', 'reading', 'time', ',', 'will', 'be', 'helpful', 'in', 'research', 'and', 'will', 'help', 'in', 'finding', 'more', 'information', 'in', 'less', 'time', '.']\n",
            "{'The importance of data to the current world can never be undermined.': 21, 'As per IDC, the total amount of data that is expected to sprout by 2025 is 180 zettabytes.': 101, 'There is no need to emphasize on the importance of text summarization in the age of big data.': 114, 'The algorithms or techniques that shorten the longer text and delivers that accurate but brief information are much needed.': 299, 'No need to say that, Text summarization will reduce the reading time, will be helpful in research and will help in finding more information in less time.': 406}\n",
            "\n",
            "\n",
            "Summary of Text:\n",
            "\n",
            "\n",
            " The algorithms or techniques that shorten the longer text and delivers that accurate but brief information are much needed. No need to say that, Text summarization will reduce the reading time, will be helpful in research and will help in finding more information in less time.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}